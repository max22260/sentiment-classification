test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test <- as.matrix(test)
View(test)
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=2 , max=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed "))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test <- as.matrix(test)
View(test)
dl <- "\\t\\r\\n.!?,;\"()"""
dl <- "\\t\\r\\n.!?,;\"()"
s<-NGramTokenizer(a,Weka_control(min=2 , max=2,delmititers = dl))
s<-NGramTokenizer(a,Weka_control(min=2 , max=2,deLmititers = dl))
s<-NGramTokenizer(a,Weka_control(min=2 , max=2, delimiters = dl))
s<-NGramTokenizer(a,Weka_control(min=2 , max=2))
s<-NGramTokenizer(a,Weka_control(min=2 , max=2))
s<-NGramTokenizer(a,Weka_control(min=2 , max=2))
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=2 , max=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed "))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test <- as.matrix(test)
View(test)
test <- as.matrix(table(test))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
----------------------------------------------------------
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=2 , max=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed "))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test <- as.matrix(table(test))
----------------------------------------------------------
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=2 , max=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed "))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test <- as.matrix(table(test))
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=2 , max=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed "))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test <- as.matrix(table(test))
test <- as.data.frame(table(test))
test1 <- data.frame(table(test))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test1 <- data.frame(table(test))
gc()
gc()
gc()
gc()
install.packages("qunteda")
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(n=2))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
load(file = "/home/max/Desktop/test/bbc_DataMatrix.RData")
bbc.data.matrix$content <-iconv(bbc.data.matrix$content,"WINDOWS-1252","UTF-8")
#--------------------------- prepare training_dataset and test1_dataset , test2_dataset -----------------
set.seed(100) # for randmnes
trian <- createDataPartition(y=bbc.data.matrix$class,p=0.70 , list = FALSE)
train_dataset <- bbc.data.matrix[trian,]
testdataset <- bbc.data.matrix[-trian,]
#---------------------------------------------------------------
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(n=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed  no one will know"))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
#-------------------------------------------------------------------------------
test
train.tokens <- as.matrix(test)
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=3 , max=3))
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=3 , max=3))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
library(SnowballC)
library(textreadr)
library(wordcloud)
library(rpart)
library(caret)
library(tm)
library(MASS)
library(RWeka)
library(rminer)
library(kernlab)
load(file = "/home/max/Desktop/test/bbc_DataMatrix.RData")
bbc.data.matrix$content <-iconv(bbc.data.matrix$content,"WINDOWS-1252","UTF-8")
#--------------------------- prepare training_dataset and test1_dataset , test2_dataset -----------------
set.seed(100) # for randmnes
trian <- createDataPartition(y=bbc.data.matrix$class,p=0.70 , list = FALSE)
train_dataset <- bbc.data.matrix[trian,]
testdataset <- bbc.data.matrix[-trian,]
#---------------------------------------------------------------
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=3 , max=3))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
#-------------------------------------------------------------------------------
test
train.tokens <- as.matrix(test)
View(train.tokens)
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(n=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
train.tokens <- as.matrix(test)
test
View(train.tokens)
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
#--------------------------- prepare training_dataset and test1_dataset , test2_dataset -----------------
set.seed(100) # for randmnes
trian <- createDataPartition(y=bbc.data.matrix$class,p=0.70 , list = FALSE)
train_dataset <- bbc.data.matrix[trian,]
testdataset <- bbc.data.matrix[-trian,]
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
test <- DocumentTermMatrix(a, control = list(tokenize = TrigramTokenizer))
x <-tokenizers::tokenize_ngrams(a,2)
x
x <-tokenizers::tokenize_ngrams(x = a , lowercase = TRUE, n = 2)
test <- DocumentTermMatrix(a, control = list(tokenize = x))
train.tokens <- as.matrix(test)
View(train.tokens)
train.tokens
x
test <- TermDocumentMatrix(a, control = list(tokenize = x))
train.tokens <- as.matrix(test)
View(train.tokens)
x <-tokenizers::tokenize_ngrams(x = a , lowercase = TRUE, n = 2 , ngram_delim = "-")
x
test <- DocumentTermMatrix(a, control = list(tokenize = x))
train.tokens <- as.matrix(test)
test
View(train.tokens)
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
x <-tokenizers::tokenize_ngrams(x = a , lowercase = TRUE, n = 2 , ngram_delim = "_")
test <- DocumentTermMatrix(a, control = list(tokenize = x))
#-------------------------------------------------------------------------------
test
train.tokens <- as.matrix(test)
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
x <-tokenizers::tokenize_ngrams(x = a , lowercase = TRUE, n = 2 , ngram_delim = "_")
test <- DocumentTermMatrix(a, control = list(tokenize = x))
#-------------------------------------------------------------------------------
test
train.tokens <- as.matrix(test)
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
x <-tokenizers::tokenize_ngrams(x = a , lowercase = TRUE, n = 2 , ngram_delim = "_")
test <- DocumentTermMatrix(a, control = list(tokenize = x))
#-------------------------------------------------------------------------------
test
train.tokens <- as.matrix(test)
View(train.tokens)
test$dimnames
test$i
test$
train.tokens <- as.matrix(test)
x <-tokenizers::tokenize_ngrams(x = train_dataset, n = 2 , ngram_delim = "_")
train_corpus <- (VectorSource(train_dataset))
x <-tokenizers::tokenize_ngrams(x = train_corpus, n = 2 , ngram_delim = "_")
train_corpus
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
FourgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
test <- DocumentTermMatrix(a, control = list(tokenize = BigramTokenizer()))
test <- DocumentTermMatrix(a, control = list(tokenize = BigramTokenizer)
#-------------------------------------------------------------------------------
train.tokens <- as.matrix(test)
test <- DocumentTermMatrix(a, control = list(tokenize = BigramTokenizer))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
train_corpus <- (VectorSource(train_dataset))
test <- DocumentTermMatrix(a, control = list(tokenize = BigramTokenizer))
#-------------------------------------------------------------------------------
train.tokens <- as.matrix(test)
View(train.tokens)
x <- ngram_tokenizer(2)(a)
ngram_tokenizer <- function(n = 1L, skip_word_none = TRUE) {
stopifnot(is.numeric(n), is.finite(n), n > 0)
options <- stringi::stri_opts_brkiter(type="word", skip_word_none = skip_word_none)
function(x) {
stopifnot(is.character(x))
# Split into word tokens
tokens <- unlist(stringi::stri_split_boundaries(x, opts_brkiter=options))
len <- length(tokens)
if(all(is.na(tokens)) || len < n) {
# If we didn't detect any words or number of tokens is less than n return empty vector
character(0)
} else {
sapply(
1:max(1, len - n + 1),
function(i) stringi::stri_join(tokens[i:min(len, i + n - 1)], collapse = " ")
)
}
}
}
x <- ngram_tokenizer(2)(a)
x <- ngram_tokenizer(2)(train_dataset$content)
x
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
install.packages("NLP")
install.packages("NLP")
library(NLP)
library(NLP)
library(NLP)
install.packages("NLP")
install.packages("NLP")
install.packages(c("biclust", "cairoDevice", "caret", "Cubist", "CVST", "data.table", "etm", "highr", "htmlTable", "labelled", "later", "mice", "munsell", "mvtnorm", "partykit", "pdftools", "pillar", "plotmo", "plotrix", "progress", "purrr", "RcppRoll", "RGtk2", "rJava", "rlang", "rmarkdown", "RWeka", "stringi", "textshape", "tidytext", "utf8", "visNetwork", "xgboost", "zoo"))
install.packages(c("bindr", "bindrcpp", "caret", "CVST", "ddalpha", "dplyr", "DRR", "kernlab", "lava", "lubridate", "munsell", "pillar", "prodlim", "purrr", "Rcpp", "RcppRoll", "recipes", "reshape2", "rJava", "rlang", "robustbase", "sfsmisc", "stringi", "stringr", "tibble", "utf8", "withr"), lib="/usr/lib/R/site-library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib="/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
load(file = "/home/max/Desktop/test/bbc_DataMatrix.RData")
View(bbc.data.matrix)
library(shiny, dependencies=TRUE)
library(shiny)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
library(shiny) # shiny library
# begining of ui component
ui<-shinyUI(fluidPage(
fluidRow(
column(12,
"Model Selection Panel",
sidebarLayout(
sidebarPanel(
h3('choose the model'),
# the actioButton called rpart which is the name of the variable you need to use in the server component
actionButton('rpart', label = 'Decision Tree',icon("leaf",lib="glyphicon"),
style="color: #fff; background-color: #339933; border-color: #2e6da4"),
# the training sample split you allow the user to control on your model
numericInput("ratio", "training sample in %", value=50/100, min = 50/100, max = 90/100, step=0.1)
),
# this is how you create many "tabs" for the output from ML models
mainPanel(
tabsetPanel( ,
tabPanel("first 5 rows of the dataframe", verbatimTextOutput("head")),
tabPanel("model result", tableOutput("result")),
tabPanel("model plot", plotOutput('plot')),
tabPanel("model summary", verbatimTextOutput('summary'))
)
)
)))
))
# all the libraries you need for your machine learning models and plots
library(rpart)				        # Popular decision tree algorithm
library(rattle)					# Fancy tree plot
library(rpart.plot)				# Enhanced tree plots
library(RColorBrewer)				# Color selection for fancy tree plot
library(party)					# Alternative decision tree algorithm
library(partykit)				# Convert rpart object to BinaryTree
library(tree) # good to have but not necessary
data(iris) # you call the famous machine learning data iris like this
# begining of your server component
server<- function(input,output, session){
set.seed(1234)
observe({
# this is how you fetch the input variable=ratio from ui component ÿ
r<-as.numeric(input$ratio)
# construct your train and test set for machine learning models
ind <- sample(2, nrow(iris), replace = TRUE, prob=c(r,1-r))
trainset = iris[ind==1,]
testset = iris[ind==2,]
# decision tree action button
observeEvent(input$rpart, {
ml_rpart<-rpart(trainset$Species~.,method='class',data=trainset,control=rpart.control(minsplit=10,cp=0))
model_pred<-predict(ml_rpart, testset, type="class")
output$result<-renderTable({
table(model_pred, testset$Species)    })
output$summary <- renderPrint(summary(ml_rpart))
output$plot <- renderPlot({
prune.fit<-prune(ml_rpart, cp=0.001)
# prune the treefirst then plot the pruned tree
plot(prune.fit, uniform=TRUE,
main="Pruned Classification Tree for iris data")
text(prune.fit, use.n=TRUE, all=TRUE, cex=.8)
})
})
#print dataframe's sample head
output$head <- renderPrint({
head(testset, 5)
})
})
}
shinyApp(ui=ui, server=server) # you call shiny all like this
library(shiny) # shiny library
# begining of ui component
ui<-fluidPage(
#  input  : stuff you allow the users to interact with:actionButton, numericInput
)
server<-function(input, output){
# fetch the input from ui and respond with some output: renderPlot, renderPrint...etc
}
# call shinyApp and launch it
shinyApp(ui=ui, server=server)
ui<-shinyUI(fluidPage(
fluidRow(
column(12,
"Model Selection Panel",
sidebarLayout(
sidebarPanel(
h3('choose the model'),
# the actioButton called rpart which is the name of the variable you need to use in the server component
actionButton('rpart', label = 'Decision Tree',icon("leaf",lib="glyphicon"),
style="color: #fff; background-color: #339933; border-color: #2e6da4"),
# the training sample split you allow the user to control on your model
numericInput("ratio", "training sample in %", value=50/100, min = 50/100, max = 90/100, step=0.1)
),
# this is how you create many "tabs" for the output from ML models
mainPanel(
tabsetPanel( ,
tabPanel("first 5 rows of the dataframe", verbatimTextOutput("head")),
tabPanel("model result", tableOutput("result")),
tabPanel("model plot", plotOutput('plot')),
tabPanel("model summary", verbatimTextOutput('summary'))
)
)
)))
))
# all the libraries you need for your machine learning models and plots
library(rpart)				        # Popular decision tree algorithm
library(rattle)					# Fancy tree plot
library(rpart.plot)				# Enhanced tree plots
library(RColorBrewer)				# Color selection for fancy tree plot
library(party)					# Alternative decision tree algorithm
library(partykit)				# Convert rpart object to BinaryTree
library(tree) # good to have but not necessary
data(iris) # you call the famous machine learning data iris like this
# begining of your server component
server<- function(input,output, session){
set.seed(1234)
observe({
# this is how you fetch the input variable=ratio from ui component ÿ
r<-as.numeric(input$ratio)
# construct your train and test set for machine learning models
ind <- sample(2, nrow(iris), replace = TRUE, prob=c(r,1-r))
trainset = iris[ind==1,]
testset = iris[ind==2,]
# decision tree action button
observeEvent(input$rpart, {
ml_rpart<-rpart(trainset$Species~.,method='class',data=trainset,control=rpart.control(minsplit=10,cp=0))
model_pred<-predict(ml_rpart, testset, type="class")
output$result<-renderTable({
table(model_pred, testset$Species)    })
output$summary <- renderPrint(summary(ml_rpart))
output$plot <- renderPlot({
prune.fit<-prune(ml_rpart, cp=0.001)
# prune the treefirst then plot the pruned tree
plot(prune.fit, uniform=TRUE,
main="Pruned Classification Tree for iris data")
text(prune.fit, use.n=TRUE, all=TRUE, cex=.8)
})
})
#print dataframe's sample head
output$head <- renderPrint({
head(testset, 5)
})
})
}
shinyApp(ui=ui, server=server) # you call shiny all like this
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
library(shiny)
ui<-fluidPage(
#  input  : stuff you allow the users to interact with:actionButton, numericInput
)
server<-function(input, output){
# fetch the input from ui and respond with some output: renderPlot, renderPrint...etc
}
# call shinyApp and launch it
shinyApp(ui=ui, server=server)
library(shiny)
ui<-fluidPage(
#  input  : stuff you allow the users to interact with:actionButton, numericInput
)
server<-function(input, output){
# fetch the input from ui and respond with some output: renderPlot, renderPrint...etc
}
# call shinyApp and launch it
shinyApp(ui=ui, server=server)
getwd(\)
getwd()
setwd("Desktop/spring2018/txt_mining/")
load("dataset.RData")
data.matrix <- all_data
remove(all_data)
data.matrix$content <-iconv(data.matrix$content,"WINDOWS-1252","UTF-8")
#-----------------------------------------------------------------------------------------------------
preProcess.N_grams <- function(row.data, stopword.dir, BagOfWord , boolstemm  , MIN_ngram , MAX_ngram , weighttype){
packages <- c('tm','RWeka' )
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
library(RWeka)
library(tm)
row.data<-iconv(row.data,"WINDOWS-1252","UTF-8")
stopwordlist<- readLines(stopword.dir)
train_corpus<- VCorpus(VectorSource(row.data))
train_corpus <-tm_map(train_corpus,content_transformer(tolower))
train_corpus<-tm_map(train_corpus,removeNumbers)
train_corpus<-tm_map(train_corpus,removeWords,stopwords("english"))
train_corpus<-tm_map(train_corpus,removeWords,stopwordlist)
train_corpus<- tm_map(train_corpus,removePunctuation)
train_corpus<-tm_map(train_corpus,stripWhitespace)
if(boolstemm){
train_corpus<-tm_map(train_corpus,stemDocument,language = "english")
}
Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = MIN_ngram, max =MAX_ngram ))
if(weighttype == "TFIDF"){
DTM <-DocumentTermMatrix(train_corpus,
control = list(tolower = T ,
removeNumbers =T ,
removePunctuation = T ,
stopwords = T
, stripWhitespace = T ,
dictionary = BagOfWord,
weighting = weightTfIdf,
tokenize = Tokenizer))
}else if(weighttype == "TF"){
DTM <-DocumentTermMatrix(train_corpus,
control = list(tolower = T ,
removeNumbers =T ,
removePunctuation = T ,
stopwords = T
, stripWhitespace = T ,
dictionary = BagOfWord,
weighting = weightTf,
tokenize = Tokenizer))
}else if(weighttype == "BIN"){
DTM <-DocumentTermMatrix(train_corpus,
control = list(tolower = T ,
removeNumbers =T ,
removePunctuation = T ,
stopwords = T
, stripWhitespace = T ,
dictionary = BagOfWord,
weighting = weightBin,
tokenize = Tokenizer))
}else{
DTM <-DocumentTermMatrix(train_corpus,
control = list(tolower = T ,
removeNumbers =T ,
removePunctuation = T ,
stopwords = T
, stripWhitespace = T ,
dictionary = BagOfWord,
tokenize = Tokenizer))
}
print("DTM DONE !!")
print(DTM)
return(DTM)
}
#----------------------------------------------------------------------------------------------------
train_dtm <-preProcess.N_grams(row.data = data.matrix$content ,stopword.dir = "stopword.txt",
BagOfWord = NULL , TRUE , MIN_ngram = 1  , MAX_ngram = 2,weighttype = "TFIdF" )
train_dtm <- removeSparseTerms(train_dtm,0.993)
BagOW <- findFreqTerms(train_dtm)
dim(train_dtm)
train_matrix <- as.matrix(train_dtm)
#train_matrix <- binary.weight(train_matrix)
train_data_model <- as.data.frame(train_matrix)
train_data_model <- data.frame(y=data.matrix$lable , x = train_data_model)
#----------------------------------------------------------------------------------------------------
remove(data.matrix)
remove(train_dtm)
remove(train_matrix)
#----------------------------------------------------------------------------------------------------
#----------------------------------------------------------------------------------------------------
read.dir <-function(dir , pattern){
file.names <- dir(dir, pattern = pattern)
file.names = as.data.frame(file.names)
file.names$content = NA
colnames(file.names) = c("filename", "content" )
for(i in 1:length(file.names[,1])){
path <- paste0(dir,'/',file.names[i,1])
line <-  readLines(path)
# print(file.names[i,1])
file.names[i,2] <-paste(line, sep = "", collapse = "")
}
return(file.names)
}
#------------------------------------------------------------------------------------------------------
library(caret)
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
timr <- Sys.time()
svm <- train(y~.,data = train_data_model,method = 'svmLinear3')
stopCluster(cl)
total_runtime <- difftime(Sys.time(), timr)
print(total_runtime)
#------------------------------------------------------------------------------------------------------
external_data <- read.dir("test/",".txt")
external_corpus <- Corpus(VectorSource(as.matrix(external_data$content)))
ex_dtm <-preProcess.N_grams(row.data = external_data$content ,stopword.dir = "stopword.txt",
BagOfWord = BagOW , TRUE , MIN_ngram = 1  , MAX_ngram = 3,weighttype = "TF" )
dim(ex_dtm)
### test1 matrix form
ex_matrix <- as.matrix(ex_dtm)
ex_data_model <- as.data.frame(ex_matrix)
##test1_data_model
dim(ex_data_model)
ex_data_model <- data.frame(x=ex_data_model)
ex_pred = predict(svm,newdata = ex_data_model ,type = "raw")
ex_pred <- ifelse(ex_pred %in% "positive", 1,0)
Sample <- read.csv(file = "sample.csv" ,header = TRUE)
Sample$labels <- ex_pred
write.csv(x = Sample ,file = "output/sample.csv")
